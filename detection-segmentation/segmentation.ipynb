{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!rm -rf cloth-segmentation\n",
    "!git clone https://github.com/levindabhi/cloth-segmentation.git\n",
    "%cd cloth-segmentation\n",
    "!gdown --id 1mhF3yqd7R-Uje092eypktNl-RoZNuiCJ\n",
    "!mkdir input_images\n",
    "!mkdir output_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from data.base_dataset import Normalize_image\n",
    "from utils.saving_utils import load_checkpoint_mgpu\n",
    "\n",
    "from networks import U2NET\n",
    "device = 'cuda'\n",
    "\n",
    "image_dir = 'path image directory'\n",
    "result_dir = 'path result directory'\n",
    "checkpoint_path = 'path checkpoint'\n",
    "\n",
    "def get_palette(num_cls):\n",
    "    \"\"\" Returns the color map for visualizing the segmentation mask.\n",
    "    Args:\n",
    "        num_cls: Number of classes\n",
    "    Returns:\n",
    "        The color map\n",
    "    \"\"\"\n",
    "    n = num_cls\n",
    "    palette = [0] * (n * 3)\n",
    "    for j in range(0, n):\n",
    "        lab = j\n",
    "        palette[j * 3 + 0] = 0\n",
    "        palette[j * 3 + 1] = 0\n",
    "        palette[j * 3 + 2] = 0\n",
    "        i = 0\n",
    "        while lab:\n",
    "            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n",
    "            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n",
    "            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n",
    "            i += 1\n",
    "            lab >>= 3\n",
    "    return palette\n",
    "\n",
    "\n",
    "transforms_list = []\n",
    "transforms_list += [transforms.ToTensor()]\n",
    "transforms_list += [Normalize_image(0.5, 0.5)]\n",
    "transform_rgb = transforms.Compose(transforms_list)\n",
    "\n",
    "net = U2NET(in_ch=3, out_ch=4)\n",
    "net = load_checkpoint_mgpu(net, checkpoint_path)\n",
    "net = net.to(device)\n",
    "net = net.eval()\n",
    "\n",
    "palette = get_palette(4)\n",
    "\n",
    "!rm -rf input_images/.ipynb_checkpoints\n",
    "images_list = sorted(os.listdir(image_dir))\n",
    "pbar = tqdm(total=len(images_list))\n",
    "outf = sorted(os.listdir(result_dir))\n",
    "\n",
    "\n",
    "for image_name in images_list:\n",
    "\n",
    "      if \"aug\" not in image_name:\n",
    "        if image_name.endswith(\"jpg\"):\n",
    "          if image_name[:-4]+'_generated.png' not in outf :\n",
    "            img = Image.open(os.path.join(image_dir, image_name)).convert('RGB')\n",
    "            img_size = img.size\n",
    "            \n",
    "            img = img.resize((768, 768), Image.BICUBIC)\n",
    "            image_tensor = transform_rgb(img)\n",
    "            image_tensor = torch.unsqueeze(image_tensor, 0)\n",
    "            \n",
    "            output_tensor = net(image_tensor.to(device))\n",
    "            output_tensor = F.log_softmax(output_tensor[0], dim=1)\n",
    "            output_tensor = torch.max(output_tensor, dim=1, keepdim=True)[1]\n",
    "            output_tensor = torch.squeeze(output_tensor, dim=0)\n",
    "            output_tensor = torch.squeeze(output_tensor, dim=0)\n",
    "            output_arr = output_tensor.cpu().numpy()\n",
    "\n",
    "            output_img = Image.fromarray(output_arr.astype('uint8'), mode='L')\n",
    "            output_img = output_img.resize(img_size, Image.BICUBIC)\n",
    "            \n",
    "            output_img.putpalette(palette)\n",
    "            output_img.save(os.path.join(result_dir, image_name[:-4]+'_generated.png'))\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "image_dir = 'path segmentation directory'\n",
    "\n",
    "orginal_dir = 'path IMAGES directory'\n",
    "\n",
    "\n",
    "result_dir = 'path result directory'\n",
    "\n",
    "images = sorted(os.listdir(image_dir))\n",
    "n = 1\n",
    "for img_file in images:\n",
    "\n",
    "\t\timage = cv2.imread(os.path.join(image_dir, img_file))\n",
    "\t\t\n",
    "\t\torginal_img = img_file[:-14]+\".jpg\"\n",
    "\t\timg = cv2.imread(os.path.join(orginal_dir , orginal_img))\n",
    "\t\t\n",
    "\t\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\t\tblurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\t\t(T, threshInv) = cv2.threshold(blurred, 10, 255,\n",
    "\t\t\tcv2.THRESH_BINARY)\n",
    "\t\tmasked = cv2.bitwise_and(img, img, mask=threshInv)\n",
    "\t\tn+=1\n",
    "\t\tcv2.imwrite(os.path.join(result_dir , orginal_img) ,masked)\n",
    "\t\tif n%5 == 0 :\n",
    "\t\t\tprint(f'{n} images done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
